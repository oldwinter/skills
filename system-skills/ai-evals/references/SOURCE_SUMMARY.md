# Source summary (Refound/Lenny)

This skill pack was derived from `sources/refound/raw/ai-evals/SKILL.md` (AI & Technology).

## Preserved core insights

1) **Evals are becoming a core “new skill” for product builders**, distinct from traditional software testing or high-level AI strategy. The intent is a repeatable workflow that starts from failures and ends in measurable evaluation artifacts (golden set, taxonomy, rubric, benchmark).

2) **If the model is the product, the eval functions like the product requirement.** The eval definition (rubrics, benchmarks, systematic tests) becomes the bottleneck and the contract for shipping quality.

## Notes / assumptions
- The source file appears truncated (several sentences cut off). This pack fills the missing operational details using standard evaluation practice: error analysis → open coding → taxonomy → rubric → harness → reporting/iteration.
- This pack is tool-agnostic and compatible with Codex/Claude Code “Agent Skills” conventions.

