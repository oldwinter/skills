# Templates (Copy/Paste)

Use these templates to produce an **AI Product Strategy Pack**.

## 0) Context snapshot (bullets)
- Product today:
- Target user/customer segment:
- Job/pain (and evidence):
- Why now:
- Decision to make (and by when):
- Strategy horizon:
- Constraints: (budget, latency, policy/legal, privacy, data access, platforms, regions)
- Success metrics (1–3):
- Guardrails (2–5): (safety, trust, cost, latency, quality)
- Stakeholders / DRIs:

## 1) Strategy thesis (1 page)
### 1.1 Decision statement
- We are deciding:
- By:
- For audience:

### 1.2 Problem and why now
- Problem (user-centered):
- Evidence:
- Why now:

### 1.3 Target user + workflow anchor
- Primary user:
- Workflow step(s) we’re changing:
- What becomes easier/faster/safer:

### 1.4 Value proposition (plain language)
- 1–2 sentences:

### 1.5 Differentiation (why us)
List 2–4 defensible levers:
- Data advantage:
- Distribution/surface area:
- Workflow integration:
- UX advantage:
- Trust advantage:

### 1.6 Strategy choices + non-goals
**Choices (we will)**
- C1:
- C2:
- C3:

**Non-goals (we will not)**
- NG1:
- NG2:
- NG3:

### 1.7 Assumptions and how we’ll test them
| Assumption | Why we believe it | How we’ll test | Metric | Timebox | Owner |
|---|---|---|---|---|---|
|  |  |  |  |  |  |

## 2) Use-case portfolio (prioritize bets)
List 6–12 candidates, then select top 1–3.

| Use case | Target user | Workflow step | Outcome metric | Feasibility (L/M/H) | Risk (L/M/H) | Data needed | “Must-not-do” constraint | Notes |
|---|---|---|---|---:|---:|---|---|---|
|  |  |  |  |  |  |  |  |  |

## 3) Autonomy policy (assistant → copilot → agent)
Define what the system can do, what it can suggest, and what it must never do.

| Capability / action | Mode (assist/suggest/act) | User approval required? | Permission scope | Logging/audit | Rollback/undo | Key failure modes | Mitigations |
|---|---|---:|---|---|---|---|---|
|  |  |  |  |  |  |  |  |

## 4) System plan (strategy-level)
### 4.1 Approach (build/buy)
- Proposed approach (RAG/tooling/fine-tune/etc.):
- Primary dependencies (vendors, platforms):
- Key unknowns to validate:

### 4.2 Data plan and governance
- Data sources we can use:
- Data sources we must not use:
- Retention and access policy assumptions:
- Privacy/compliance constraints:

### 4.3 Eval plan (offline + online)
**Offline evals (pre-ship)**
- Test set sources:
- Critical failure tests (must pass):
- Target quality bar:

**Online monitoring (post-ship)**
- Quality signals:
- Safety/trust signals:
- Escalation/override signals:
- Owner + review cadence:

### 4.4 Budgets
- Latency target:
- Cost target (per user/day or per task):
- Reliability target (timeouts/errors):

## 5) Empirical learning plan (experiments + instrumentation)
| Hypothesis | Experiment/prototype | Success metric | Guardrail metric | Instrumentation needed | Timebox | Owner | Decision rule |
|---|---|---|---|---|---|---|---|
|  |  |  |  |  |  |  |  |

## 6) Roadmap (phased, with exit criteria)
Use rollout tiers and make risk work explicit.

| Phase | Scope (what ships) | Target users | Entry criteria | Exit criteria | Key risks to retire | Owner | Target date |
|---|---|---|---|---|---|---|---|
| 0: Prototype |  |  |  |  |  |  |  |
| 1: Internal |  |  |  |  |  |  |  |
| 2: Beta |  |  |  |  |  |  |  |
| 3: GA |  |  |  |  |  |  |  |

## 7) Risks / Open questions / Next steps (always include)
### Risks
| Risk | Likelihood (L/M/H) | Impact (L/M/H) | Mitigation | Owner |
|---|---:|---:|---|---|
|  |  |  |  |  |

### Open questions
- OQ1:
- OQ2:
- OQ3:

### Next steps (1–2 weeks)
- NS1:
- NS2:
- NS3:

