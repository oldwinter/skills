# Checklists (Problem Definition)

Use these before finalizing the Problem Definition Pack.

## 1) Problem statement checklist
- 1-liner identifies **who**, **context**, **pain**, and **impact**
- Avoids embedding a solution or technology choice
- Includes **why now** (trigger/event)
- Defines the problem in terms that could be proven wrong (testable)

## 2) JTBD checklist
- Primary job is written as **When… I want… so I can…**
- Sub-jobs cover major steps (not features)
- Success criteria are from the user’s perspective (not internal KPIs)

## 3) Alternatives + switching checklist
- Includes what users do today (including “do nothing”)
- Includes at least one analog/non-digital alternative when relevant
- Explains why current alternatives work (not just why they’re bad)
- Calls out switching costs (workflow change, trust, data migration, learning)
- Answers: “Why would a user give this the time of day?”

## 4) Evidence & assumptions checklist
- Separates **knowns** from **hypotheses**
- Each key claim has evidence or an explicit “unknown” label
- Top 1–3 riskiest assumptions have a concrete test plan
- Avoids the “shiny object trap” (tech choice follows validated pain)

## 5) Success + guardrails checklist
- Outcome metric(s) reflect user value delivered
- Leading indicators are controllable in weeks/months
- Guardrails protect against harm (trust, quality, cost, support load, latency)
- Metrics are defined unambiguously (no interpretation disputes)

## 6) Prototype/learning plan checklist
- The team can describe the end state (what “done” looks like)
- Prototype is the **fastest** path to learning (not a mini-build)
- Success criteria for the test are defined in advance
- Includes timeline, audience, and key questions

## 7) Finalization checklist
- Includes **Risks / Open questions / Next steps** with owners
- Assumptions are labeled and reviewable
- A stakeholder can decide “proceed/pause/stop” async

