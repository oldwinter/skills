# Rubric (Score 1–5)

Score the User Feedback Analysis Pack. Target: **≥ 20/30** overall, with no category below **3** for high-stakes decisions.

## 1) Decision clarity (1–5)
1: No decision; vague “insights” request.  
3: Decision stated; scope and audience mostly clear.  
5: Decision + deadline + success criteria are explicit; scope boundaries are crisp.

## 2) Data coverage + sampling rigor (1–5)
1: Single anecdote stream; no sampling explanation.  
3: Multiple sources or a documented sample; limitations stated.  
5: Clear inventory + representative sampling; segment coverage and caveats are explicit.

## 3) Taxonomy/codebook usability (1–5)
1: Tags are unclear or inconsistent.  
3: Clear definitions; workable tag set; severity scale present.  
5: Codebook enables consistent tagging; includes/excludes + examples; supports the decision.

## 4) Theme quality + evidence traceability (1–5)
1: Themes are generic and lack evidence.  
3: Themes have quotes/examples and basic quantification.  
5: Themes are specific, well-evidenced, quantified appropriately, and split by segment/source where useful.

## 5) Actionability of recommendations (1–5)
1: Recommendations are vague (“improve onboarding”).  
3: Concrete actions tied to themes; some prioritization.  
5: Ranked actions with owners/time horizons, expected impact, and a clear learning plan for unknowns.

## 6) Durability + loop design (1–5)
1: One-off report with no follow-up plan.  
3: Basic cadence and storage plan.  
5: Clear operating model (cadence, owners, engineering participation) and a plan to prevent duplicated research.

