# Rubric (Score the AI Evals Pack)

Score each category from 1 (poor) to 5 (excellent). A “ship-ready” pack typically averages ≥4 with no category below 3.

## 1) Clarity of decision + scope
1: Vague goal; no decision or non-goals.  
3: Decision is stated; scope mostly clear; a few ambiguities remain.  
5: Decision, scope, and non-goals are crisp; stakeholders can act immediately.

## 2) Test set quality (representativeness + coverage)
1: Mostly happy paths; little tagging/coverage logic.  
3: Covers main scenarios; some tagging and edge cases; gaps remain.  
5: High-signal cases with clear schema and strong coverage across critical segments/risks.

## 3) Taxonomy usefulness
1: Generic categories; not actionable.  
3: Reasonable categories; some actionable guidance.  
5: Clear, specific, severity-weighted categories that map to fixes and new tests.

## 4) Rubric executability
1: Subjective/vibes; judges cannot apply consistently.  
3: Mostly clear; still some ambiguity.  
5: Behaviorally anchored; clear scoring, examples, tie-breakers; high judge agreement expected.

## 5) Judge + harness repeatability
1: No runbook; not reproducible.  
3: Repeatable at a basic level; some versioning/cost gaps.  
5: Fully repeatable and auditable: versioned artifacts, calibration, cost/time estimates, clear data handling.

## 6) Reporting + iteration loop actionability
1: Results don’t translate into action.  
3: Results are interpretable; next steps exist.  
5: Decision-ready reporting with regression rules; iteration loop turns failures into prioritized work.

## Required final section
The pack must include: **Risks**, **Open questions**, **Next steps**.

